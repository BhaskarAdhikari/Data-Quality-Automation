{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2cd6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote CSVs to: c:\\Users\\Bhaskar\\Git\\Automation\\data\\raw\n",
      "Files:\n",
      " - suppliers.csv: 3,734 bytes\n",
      " - parts.csv: 11,635 bytes\n",
      " - purchase_orders.csv: 39,068 bytes\n",
      " - po_lines.csv: 126,310 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhaskar\\AppData\\Local\\Temp\\ipykernel_37800\\3021821242.py:93: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'yes' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  parts.loc[np.random.choice(parts.index, size=2, replace=False), \"is_active\"] = \"yes\"\n"
     ]
    }
   ],
   "source": [
    "# generate_synthetic_procurement_data.py\n",
    "# Creates 4 CSVs with synthetic procurement data and intentionally \"bad\" data.\n",
    "# Outputs to: ./data/raw/\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def random_date(start: datetime, end: datetime) -> datetime:\n",
    "    \"\"\"Random datetime between start and end.\"\"\"\n",
    "    delta = end - start\n",
    "    seconds = random.randint(0, int(delta.total_seconds()))\n",
    "    return start + timedelta(seconds=seconds)\n",
    "\n",
    "\n",
    "def main(\n",
    "    out_dir: str = \"data/raw\",\n",
    "    seed: int = 42,\n",
    "    n_suppliers: int = 120,\n",
    "    n_parts: int = 350,\n",
    "    n_pos: int = 900,\n",
    "    n_lines: int = 3200,\n",
    ") -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # ---------- Reference values ----------\n",
    "    countries = [\"US\", \"CA\", \"MX\", \"GB\", \"DE\", \"FR\", \"IN\", \"CN\", \"JP\", \"BR\", \"AU\"]\n",
    "    supplier_statuses = [\"ACTIVE\", \"INACTIVE\", \"BLOCKED\"]\n",
    "    part_types = [\"RAW\", \"FINISHED\", \"PACKAGING\", \"SERVICE\", \"MRO\"]\n",
    "    uoms = [\"EA\", \"KG\", \"LB\", \"L\", \"M\", \"BOX\"]\n",
    "    po_statuses = [\"OPEN\", \"APPROVED\", \"CLOSED\", \"CANCELLED\"]\n",
    "\n",
    "    # \"Good\" currency list + some wrong ones weâ€™ll inject later\n",
    "    good_currencies = [\"USD\", \"EUR\", \"GBP\", \"CAD\", \"JPY\", \"INR\", \"AUD\"]\n",
    "    bad_currency_codes = [\"US\", \"EURO\", \"usd\", \"XXX\", \"\", None]\n",
    "\n",
    "    # ---------- suppliers.csv ----------\n",
    "    supplier_ids = np.arange(1000, 1000 + n_suppliers)\n",
    "\n",
    "    suppliers = pd.DataFrame({\n",
    "        \"supplier_id\": supplier_ids,\n",
    "        \"supplier_name\": [f\"Supplier {i}\" for i in supplier_ids],\n",
    "        \"status\": np.random.choice(supplier_statuses, size=n_suppliers, p=[0.75, 0.18, 0.07]),\n",
    "        \"country\": np.random.choice(countries, size=n_suppliers),\n",
    "    })\n",
    "\n",
    "    # Seed bad data: nulls, duplicates, invalid values\n",
    "    # 1) Null supplier_name\n",
    "    suppliers.loc[np.random.choice(suppliers.index, size=3, replace=False), \"supplier_name\"] = None\n",
    "    # 2) Duplicate supplier_id rows (exact duplicates)\n",
    "    dup_rows = suppliers.sample(2, random_state=seed)\n",
    "    suppliers = pd.concat([suppliers, dup_rows], ignore_index=True)\n",
    "    # 3) Invalid status\n",
    "    suppliers.loc[np.random.choice(suppliers.index, size=2, replace=False), \"status\"] = \"ACTIV\"  # typo\n",
    "    # 4) Invalid country code length\n",
    "    suppliers.loc[np.random.choice(suppliers.index, size=2, replace=False), \"country\"] = \"UNITED_STATES\"\n",
    "\n",
    "    suppliers.to_csv(os.path.join(out_dir, \"suppliers.csv\"), index=False)\n",
    "\n",
    "    # ---------- parts.csv ----------\n",
    "    part_ids = np.arange(20000, 20000 + n_parts)\n",
    "\n",
    "    parts = pd.DataFrame({\n",
    "        \"part_id\": part_ids,\n",
    "        \"part_name\": [f\"Part {pid}\" for pid in part_ids],\n",
    "        \"part_type\": np.random.choice(part_types, size=n_parts),\n",
    "        \"uom\": np.random.choice(uoms, size=n_parts),\n",
    "        \"is_active\": np.random.choice([True, False], size=n_parts, p=[0.9, 0.1]),\n",
    "    })\n",
    "\n",
    "    # Seed bad data\n",
    "    # 1) Null uom\n",
    "    parts.loc[np.random.choice(parts.index, size=4, replace=False), \"uom\"] = None\n",
    "    # 2) Duplicate part_id\n",
    "    parts = pd.concat([parts, parts.sample(3, random_state=seed)], ignore_index=True)\n",
    "    # 3) Invalid UOM\n",
    "    parts.loc[np.random.choice(parts.index, size=3, replace=False), \"uom\"] = \"EACH\"\n",
    "    # 4) Invalid is_active (string)\n",
    "    parts.loc[np.random.choice(parts.index, size=2, replace=False), \"is_active\"] = \"yes\"\n",
    "\n",
    "    parts.to_csv(os.path.join(out_dir, \"parts.csv\"), index=False)\n",
    "\n",
    "    # ---------- purchase_orders.csv ----------\n",
    "    po_ids = np.arange(500000, 500000 + n_pos)\n",
    "\n",
    "    start = datetime.now() - timedelta(days=365 * 2)\n",
    "    end = datetime.now()\n",
    "\n",
    "    purchase_orders = pd.DataFrame({\n",
    "        \"po_id\": po_ids,\n",
    "        \"supplier_id\": np.random.choice(supplier_ids, size=n_pos),\n",
    "        \"po_date\": [random_date(start, end).date().isoformat() for _ in range(n_pos)],\n",
    "        \"currency\": np.random.choice(good_currencies, size=n_pos, p=[0.55, 0.12, 0.08, 0.1, 0.05, 0.07, 0.03]),\n",
    "        \"total_amount\": np.round(np.random.gamma(shape=2.0, scale=1200.0, size=n_pos), 2),\n",
    "        \"status\": np.random.choice(po_statuses, size=n_pos, p=[0.25, 0.45, 0.25, 0.05]),\n",
    "    })\n",
    "\n",
    "    # Seed bad data\n",
    "    # 1) Null currency\n",
    "    purchase_orders.loc[np.random.choice(purchase_orders.index, size=5, replace=False), \"currency\"] = None\n",
    "    # 2) Bad currency codes\n",
    "    purchase_orders.loc[np.random.choice(purchase_orders.index, size=6, replace=False), \"currency\"] = np.random.choice(\n",
    "        bad_currency_codes, size=6\n",
    "    )\n",
    "    # 3) Negative total_amount\n",
    "    purchase_orders.loc[np.random.choice(purchase_orders.index, size=4, replace=False), \"total_amount\"] *= -1\n",
    "    # 4) Duplicate po_id rows\n",
    "    purchase_orders = pd.concat([purchase_orders, purchase_orders.sample(3, random_state=seed)], ignore_index=True)\n",
    "    # 5) Supplier_id that does NOT exist (referential integrity break)\n",
    "    purchase_orders.loc[np.random.choice(purchase_orders.index, size=3, replace=False), \"supplier_id\"] = 999999\n",
    "\n",
    "    purchase_orders.to_csv(os.path.join(out_dir, \"purchase_orders.csv\"), index=False)\n",
    "\n",
    "    # ---------- po_lines.csv ----------\n",
    "    # Make lines by sampling po_id + part_id, then computing amounts.\n",
    "    po_line_ids = np.arange(9000000, 9000000 + n_lines)\n",
    "\n",
    "    po_lines = pd.DataFrame({\n",
    "        \"po_line_id\": po_line_ids,\n",
    "        \"po_id\": np.random.choice(po_ids, size=n_lines),\n",
    "        \"part_id\": np.random.choice(part_ids, size=n_lines),\n",
    "        \"qty\": np.random.randint(1, 50, size=n_lines).astype(float),\n",
    "        \"unit_price\": np.round(np.random.lognormal(mean=3.0, sigma=0.6, size=n_lines), 2),\n",
    "    })\n",
    "    po_lines[\"line_amount\"] = np.round(po_lines[\"qty\"] * po_lines[\"unit_price\"], 2)\n",
    "\n",
    "    # Seed bad data\n",
    "    # 1) Null qty and unit_price\n",
    "    po_lines.loc[np.random.choice(po_lines.index, size=6, replace=False), \"qty\"] = np.nan\n",
    "    po_lines.loc[np.random.choice(po_lines.index, size=6, replace=False), \"unit_price\"] = np.nan\n",
    "    # 2) Negative qty\n",
    "    po_lines.loc[np.random.choice(po_lines.index, size=6, replace=False), \"qty\"] = -np.random.randint(1, 20, size=6)\n",
    "    # 3) Negative unit_price\n",
    "    po_lines.loc[np.random.choice(po_lines.index, size=4, replace=False), \"unit_price\"] *= -1\n",
    "    # 4) Incorrect line_amount (doesn't match qty * unit_price)\n",
    "    idx_bad_amount = np.random.choice(po_lines.index, size=8, replace=False)\n",
    "    po_lines.loc[idx_bad_amount, \"line_amount\"] = np.round(\n",
    "        po_lines.loc[idx_bad_amount, \"line_amount\"] * np.random.uniform(0.3, 1.8, size=8), 2\n",
    "    )\n",
    "    # 5) Duplicate po_line_id\n",
    "    po_lines = pd.concat([po_lines, po_lines.sample(5, random_state=seed)], ignore_index=True)\n",
    "    # 6) po_id that does NOT exist\n",
    "    po_lines.loc[np.random.choice(po_lines.index, size=4, replace=False), \"po_id\"] = 123456789\n",
    "    # 7) part_id that does NOT exist\n",
    "    po_lines.loc[np.random.choice(po_lines.index, size=4, replace=False), \"part_id\"] = 88888888\n",
    "\n",
    "    po_lines.to_csv(os.path.join(out_dir, \"po_lines.csv\"), index=False)\n",
    "\n",
    "    print(f\"Done. Wrote CSVs to: {os.path.abspath(out_dir)}\")\n",
    "    print(\"Files:\")\n",
    "    for f in [\"suppliers.csv\", \"parts.csv\", \"purchase_orders.csv\", \"po_lines.csv\"]:\n",
    "        path = os.path.join(out_dir, f)\n",
    "        print(f\" - {f}: {os.path.getsize(path):,} bytes\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daeceb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_csvs(data_dir: str | Path) -> dict[str, pd.DataFrame]:\n",
    "    data_dir = Path(data_dir)\n",
    "    dfs = {}\n",
    "    for name in [\"suppliers\", \"parts\", \"purchase_orders\", \"po_lines\"]:\n",
    "        path = data_dir / f\"{name}.csv\"\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "        dfs[name] = pd.read_csv(path)\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "283f6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class CheckResult:\n",
    "    check_name: str\n",
    "    table: str\n",
    "    severity: str  # \"CRITICAL\" | \"WARN\"\n",
    "    passed: bool\n",
    "    failed_count: int\n",
    "    sample_failures: list[dict[str, Any]]\n",
    "\n",
    "def results_to_json(results: list[CheckResult]) -> list[dict[str, Any]]:\n",
    "    return [asdict(r) for r in results]\n",
    "\n",
    "def results_to_dataframe(results: list[CheckResult]) -> pd.DataFrame:\n",
    "    return pd.DataFrame([asdict(r) for r in results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d379100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Iterable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _sample_rows(df: pd.DataFrame, mask: pd.Series, cols: list[str], n: int = 5) -> list[dict[str, Any]]:\n",
    "    if mask is None or mask.sum() == 0:\n",
    "        return []\n",
    "    sample = df.loc[mask, cols].head(n)\n",
    "    return sample.to_dict(orient=\"records\")\n",
    "\n",
    "def check_required_columns(df: pd.DataFrame, table: str, required: Iterable[str], severity: str = \"CRITICAL\") -> CheckResult:\n",
    "    required = list(required)\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    passed = len(missing) == 0\n",
    "    return CheckResult(\n",
    "        check_name=\"required_columns\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=passed,\n",
    "        failed_count=len(missing),\n",
    "        sample_failures=[{\"missing_column\": c} for c in missing][:5],\n",
    "    )\n",
    "\n",
    "def check_primary_key_unique(df: pd.DataFrame, table: str, pk: str, severity: str = \"CRITICAL\") -> CheckResult:\n",
    "    if pk not in df.columns:\n",
    "        return CheckResult(\"primary_key_unique\", table, severity, False, 1, [{\"error\": f\"pk column missing: {pk}\"}])\n",
    "    dup_mask = df[pk].duplicated(keep=False) & df[pk].notna()\n",
    "    failed_count = int(dup_mask.sum())\n",
    "    return CheckResult(\n",
    "        check_name=\"primary_key_unique\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=_sample_rows(df, dup_mask, [pk]),\n",
    "    )\n",
    "\n",
    "def check_not_null(df: pd.DataFrame, table: str, col: str, severity: str = \"CRITICAL\") -> CheckResult:\n",
    "    if col not in df.columns:\n",
    "        return CheckResult(\"not_null\", table, severity, False, 1, [{\"error\": f\"column missing: {col}\"}])\n",
    "    null_mask = df[col].isna()\n",
    "    failed_count = int(null_mask.sum())\n",
    "    return CheckResult(\n",
    "        check_name=f\"not_null:{col}\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=_sample_rows(df, null_mask, [col]),\n",
    "    )\n",
    "\n",
    "def check_allowed_values(df: pd.DataFrame, table: str, col: str, allowed: set[Any], severity: str = \"CRITICAL\") -> CheckResult:\n",
    "    if col not in df.columns:\n",
    "        return CheckResult(\"allowed_values\", table, severity, False, 1, [{\"error\": f\"column missing: {col}\"}])\n",
    "    bad_mask = df[col].notna() & ~df[col].isin(list(allowed))\n",
    "    failed_count = int(bad_mask.sum())\n",
    "    return CheckResult(\n",
    "        check_name=f\"allowed_values:{col}\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=_sample_rows(df, bad_mask, [col]),\n",
    "    )\n",
    "\n",
    "def check_numeric_min(df: pd.DataFrame, table: str, col: str, min_value: float, severity: str = \"CRITICAL\") -> CheckResult:\n",
    "    if col not in df.columns:\n",
    "        return CheckResult(\"numeric_min\", table, severity, False, 1, [{\"error\": f\"column missing: {col}\"}])\n",
    "    series = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    bad_mask = series.notna() & (series < min_value)\n",
    "    failed_count = int(bad_mask.sum())\n",
    "    return CheckResult(\n",
    "        check_name=f\"numeric_min:{col}\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=_sample_rows(df.assign(**{col: series}), bad_mask, [col]),\n",
    "    )\n",
    "\n",
    "def check_fk_exists(child: pd.DataFrame, parent: pd.DataFrame, table: str, fk_col: str, parent_key: str, severity: str = \"CRITICAL\") -> CheckResult:\n",
    "    if fk_col not in child.columns:\n",
    "        return CheckResult(\"fk_exists\", table, severity, False, 1, [{\"error\": f\"fk column missing: {fk_col}\"}])\n",
    "    if parent_key not in parent.columns:\n",
    "        return CheckResult(\"fk_exists\", table, severity, False, 1, [{\"error\": f\"parent key missing: {parent_key}\"}])\n",
    "\n",
    "    parent_keys = set(parent[parent_key].dropna().unique().tolist())\n",
    "    bad_mask = child[fk_col].notna() & ~child[fk_col].isin(list(parent_keys))\n",
    "    failed_count = int(bad_mask.sum())\n",
    "    return CheckResult(\n",
    "        check_name=f\"fk_exists:{fk_col}->{parent_key}\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=_sample_rows(child, bad_mask, [fk_col]),\n",
    "    )\n",
    "\n",
    "def check_line_amount_math(po_lines: pd.DataFrame, severity: str = \"CRITICAL\", tolerance: float = 0.01) -> CheckResult:\n",
    "    table = \"po_lines\"\n",
    "    needed = [\"qty\", \"unit_price\", \"line_amount\"]\n",
    "    for c in needed:\n",
    "        if c not in po_lines.columns:\n",
    "            return CheckResult(\"line_amount_math\", table, severity, False, 1, [{\"error\": f\"missing column: {c}\"}])\n",
    "\n",
    "    qty = pd.to_numeric(po_lines[\"qty\"], errors=\"coerce\")\n",
    "    unit_price = pd.to_numeric(po_lines[\"unit_price\"], errors=\"coerce\")\n",
    "    line_amount = pd.to_numeric(po_lines[\"line_amount\"], errors=\"coerce\")\n",
    "\n",
    "    expected = qty * unit_price\n",
    "    diff = (line_amount - expected).abs()\n",
    "\n",
    "    bad_mask = diff.notna() & (diff > tolerance)\n",
    "    failed_count = int(bad_mask.sum())\n",
    "\n",
    "    tmp = po_lines.copy()\n",
    "    tmp[\"_expected\"] = expected\n",
    "    tmp[\"_diff\"] = diff\n",
    "\n",
    "    return CheckResult(\n",
    "        check_name=\"line_amount_math\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=_sample_rows(tmp, bad_mask, [\"qty\", \"unit_price\", \"line_amount\", \"_expected\", \"_diff\"]),\n",
    "    )\n",
    "\n",
    "def check_po_totals_reconcile(purchase_orders: pd.DataFrame, po_lines: pd.DataFrame, severity: str = \"WARN\", tolerance: float = 0.05) -> CheckResult:\n",
    "    table = \"purchase_orders\"\n",
    "    needed_po = [\"po_id\", \"total_amount\"]\n",
    "    needed_lines = [\"po_id\", \"line_amount\"]\n",
    "    for c in needed_po:\n",
    "        if c not in purchase_orders.columns:\n",
    "            return CheckResult(\"po_totals_reconcile\", table, severity, False, 1, [{\"error\": f\"missing purchase_orders.{c}\"}])\n",
    "    for c in needed_lines:\n",
    "        if c not in po_lines.columns:\n",
    "            return CheckResult(\"po_totals_reconcile\", table, severity, False, 1, [{\"error\": f\"missing po_lines.{c}\"}])\n",
    "\n",
    "    lines_amount = pd.to_numeric(po_lines[\"line_amount\"], errors=\"coerce\")\n",
    "    po_lines2 = po_lines.copy()\n",
    "    po_lines2[\"line_amount\"] = lines_amount\n",
    "\n",
    "    sums = po_lines2.groupby(\"po_id\", dropna=False)[\"line_amount\"].sum().reset_index()\n",
    "    merged = purchase_orders[[\"po_id\", \"total_amount\"]].merge(sums, on=\"po_id\", how=\"left\", suffixes=(\"_po\", \"_lines\"))\n",
    "\n",
    "    merged[\"total_amount\"] = pd.to_numeric(merged[\"total_amount\"], errors=\"coerce\")\n",
    "    merged[\"line_amount\"] = merged[\"line_amount\"].fillna(0)\n",
    "\n",
    "    merged[\"_diff\"] = (merged[\"total_amount\"] - merged[\"line_amount\"]).abs()\n",
    "    bad_mask = merged[\"_diff\"].notna() & (merged[\"_diff\"] > tolerance)\n",
    "\n",
    "    failed_count = int(bad_mask.sum())\n",
    "    return CheckResult(\n",
    "        check_name=\"po_totals_reconcile\",\n",
    "        table=table,\n",
    "        severity=severity,\n",
    "        passed=failed_count == 0,\n",
    "        failed_count=failed_count,\n",
    "        sample_failures=merged.loc[bad_mask, [\"po_id\", \"total_amount\", \"line_amount\", \"_diff\"]].head(5).to_dict(orient=\"records\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38a01795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "\n",
    "def run_all_checks(data_dir: str | Path) -> list:\n",
    "    dfs = load_csvs(data_dir)\n",
    "    suppliers = dfs[\"suppliers\"]\n",
    "    parts = dfs[\"parts\"]\n",
    "    purchase_orders = dfs[\"purchase_orders\"]\n",
    "    po_lines = dfs[\"po_lines\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Schema\n",
    "    results.append(check_required_columns(suppliers, \"suppliers\", [\"supplier_id\", \"supplier_name\", \"status\", \"country\"]))\n",
    "    results.append(check_required_columns(parts, \"parts\", [\"part_id\", \"part_name\", \"part_type\", \"uom\", \"is_active\"]))\n",
    "    results.append(check_required_columns(purchase_orders, \"purchase_orders\", [\"po_id\", \"supplier_id\", \"po_date\", \"currency\", \"total_amount\", \"status\"]))\n",
    "    results.append(check_required_columns(po_lines, \"po_lines\", [\"po_line_id\", \"po_id\", \"part_id\", \"qty\", \"unit_price\", \"line_amount\"]))\n",
    "\n",
    "    # PK\n",
    "    results.append(check_primary_key_unique(suppliers, \"suppliers\", \"supplier_id\"))\n",
    "    results.append(check_primary_key_unique(parts, \"parts\", \"part_id\"))\n",
    "    results.append(check_primary_key_unique(purchase_orders, \"purchase_orders\", \"po_id\"))\n",
    "    results.append(check_primary_key_unique(po_lines, \"po_lines\", \"po_line_id\"))\n",
    "\n",
    "    # Not null IDs\n",
    "    results.append(check_not_null(suppliers, \"suppliers\", \"supplier_id\"))\n",
    "    results.append(check_not_null(parts, \"parts\", \"part_id\"))\n",
    "    results.append(check_not_null(purchase_orders, \"purchase_orders\", \"po_id\"))\n",
    "    results.append(check_not_null(po_lines, \"po_lines\", \"po_line_id\"))\n",
    "    results.append(check_not_null(purchase_orders, \"purchase_orders\", \"supplier_id\"))\n",
    "    results.append(check_not_null(po_lines, \"po_lines\", \"po_id\"))\n",
    "    results.append(check_not_null(po_lines, \"po_lines\", \"part_id\"))\n",
    "\n",
    "    # Allowed values\n",
    "    results.append(check_allowed_values(suppliers, \"suppliers\", \"status\", {\"Active\", \"Inactive\"}, severity=\"WARN\"))\n",
    "    results.append(check_allowed_values(parts, \"parts\", \"is_active\", {0, 1, \"0\", \"1\"}, severity=\"WARN\"))\n",
    "    results.append(check_allowed_values(purchase_orders, \"purchase_orders\", \"status\", {\"Open\", \"Closed\", \"Cancelled\"}, severity=\"WARN\"))\n",
    "    results.append(check_allowed_values(purchase_orders, \"purchase_orders\", \"currency\", {\"USD\", \"EUR\", \"GBP\", \"JPY\", \"CAD\", \"AUD\"}, severity=\"WARN\"))\n",
    "\n",
    "    # Numeric checks\n",
    "    results.append(check_numeric_min(po_lines, \"po_lines\", \"qty\", 0.000001))\n",
    "    results.append(check_numeric_min(po_lines, \"po_lines\", \"unit_price\", 0.0))\n",
    "    results.append(check_numeric_min(purchase_orders, \"purchase_orders\", \"total_amount\", 0.0, severity=\"WARN\"))\n",
    "\n",
    "    # FK checks\n",
    "    results.append(check_fk_exists(purchase_orders, suppliers, \"purchase_orders\", \"supplier_id\", \"supplier_id\"))\n",
    "    results.append(check_fk_exists(po_lines, purchase_orders, \"po_lines\", \"po_id\", \"po_id\"))\n",
    "    results.append(check_fk_exists(po_lines, parts, \"po_lines\", \"part_id\", \"part_id\"))\n",
    "\n",
    "    # Math + reconcile\n",
    "    results.append(check_line_amount_math(po_lines))\n",
    "    results.append(check_po_totals_reconcile(purchase_orders, po_lines, severity=\"WARN\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def write_reports(results: list, out_dir: str | Path) -> dict:\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    json_path = out_dir / \"validation_report.json\"\n",
    "    csv_path = out_dir / \"validation_report.csv\"\n",
    "\n",
    "    payload = results_to_json(results)\n",
    "    json_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    df = results_to_dataframe(results)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    summary = {\n",
    "        \"total_checks\": len(results),\n",
    "        \"passed\": sum(1 for r in results if r.passed),\n",
    "        \"failed\": sum(1 for r in results if not r.passed),\n",
    "        \"critical_failed\": sum(1 for r in results if (not r.passed and r.severity == \"CRITICAL\")),\n",
    "        \"json_report\": str(json_path),\n",
    "        \"csv_report\": str(csv_path),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "def exit_code(results: list) -> int:\n",
    "    critical_failed = any((not r.passed) and (r.severity == \"CRITICAL\") for r in results)\n",
    "    return 2 if critical_failed else 0\n",
    "\n",
    "def setup_logging(log_dir: str | Path) -> None:\n",
    "    log_dir = Path(log_dir)\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_path = log_dir / \"validation.log\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "        handlers=[logging.FileHandler(log_path), logging.StreamHandler()],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1634d3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"total_checks\": 27,\n",
      "  \"passed\": 11,\n",
      "  \"failed\": 16,\n",
      "  \"critical_failed\": 10,\n",
      "  \"json_report\": \"reports\\\\validation_report.json\",\n",
      "  \"csv_report\": \"reports\\\\validation_report.csv\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# For notebook execution, call functions directly with default parameters\n",
    "# instead of using argparse (which conflicts with Jupyter kernel args)\n",
    "\n",
    "data_dir = \"data/raw\"\n",
    "out_dir = \"reports\"\n",
    "log_dir = \"logs\"\n",
    "\n",
    "setup_logging(log_dir)\n",
    "\n",
    "results = run_all_checks(data_dir)\n",
    "summary = write_reports(results, out_dir)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e9e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote CSVs to: c:\\Users\\Bhaskar\\Git\\Automation\\data\n",
      "Files:\n",
      " - suppliers.csv: 3,734 bytes\n",
      " - parts.csv: 11,635 bytes\n",
      " - purchase_orders.csv: 39,068 bytes\n",
      " - po_lines.csv: 126,310 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhaskar\\AppData\\Local\\Temp\\ipykernel_37800\\3021821242.py:93: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'yes' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  parts.loc[np.random.choice(parts.index, size=2, replace=False), \"is_active\"] = \"yes\"\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic test data first\n",
    "main(out_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75349529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
